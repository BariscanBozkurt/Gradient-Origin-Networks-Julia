{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnetArray{Float32, N} where N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set display width, load packages, import symbols\n",
    "ENV[\"COLUMNS\"]=72\n",
    "using Distributions\n",
    "using Interpolations\n",
    "using Knet: Knet, dir, accuracy, progress, sgd, load143, save, gc, Param, KnetArray, Data, minibatch, nll, relu, training, dropout,sigm # param, param0, xavier_uniform\n",
    "using Knet\n",
    "using Images\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using IterTools: ncycle, takenth\n",
    "using MLDatasets\n",
    "using Base.Iterators: flatten\n",
    "import CUDA # functional\n",
    "using ImageTransformations\n",
    "using Statistics\n",
    "using Memento\n",
    "using NPZ\n",
    "# using Interpolations\n",
    "atype=(CUDA.functional() ? KnetArray{Float32} : Array{Float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const F = Float32\n",
    "params = Knet.params\n",
    "\n",
    "logger = Memento.config!(\"info\"; fmt=\"[{date} | {level} | {name}]: {msg}\");\n",
    "\n",
    "include(\"PlotUtility.jl\")\n",
    "include(\"ImageUtility.jl\")\n",
    "include(\"TrainUtility.jl\")\n",
    "include(\"LayerUtility.jl\")\n",
    "include(\"LossUtility.jl\")\n",
    "\n",
    "using .PlotUtility\n",
    "using .ImageUtility\n",
    "using .TrainUtility\n",
    "using .LayerUtility\n",
    "using .LossUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CHANGE THIS LINE FOR DATASET PARAMETER ##############################\n",
    "dataset_name = \"fashion\"\n",
    "exp_number = 1\n",
    "########################### CHANGE THIS LINE FOR RESULT FOLDER NAME #############################\n",
    "notebook_name = \"Implicit_GON_Fashion\" * \"_\" * dataset_name * string(exp_number)\n",
    "\n",
    "if !isdir(\"Results\")\n",
    "   mkdir(\"Results\") \n",
    "end\n",
    "if  !isdir(joinpath(\"Results\", notebook_name))\n",
    "    mkdir(joinpath(\"Results\", notebook_name))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Saved_Models\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Saved_Models\"))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Images\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Images\"))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Logger\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Logger\"))\n",
    "end\n",
    "\n",
    "push!(logger, DefaultHandler(joinpath(\"Results\", notebook_name, \"Logger\", \"logger.log\"),DefaultFormatter(\"[{date} | {level} | {name}]: {msg}\")));\n",
    "\n",
    "use_saved_data = false\n",
    "nc = nothing\n",
    "\n",
    "if dataset_name == \"mnist\"\n",
    "    nc = 1\n",
    "    if use_saved_data\n",
    "        xtrn = npzread(\"Data/MNIST_Train_Images.npy\")\n",
    "        xtrn = permutedims(xtrn, (3,4,2,1))\n",
    "        xtst = npzread(\"Data/MNIST_Test_Images.npy\")\n",
    "        xtst = permutedims(xtst, (3,4,2,1))\n",
    "\n",
    "    else\n",
    "\n",
    "        xtrn,_ = MNIST.traindata()\n",
    "        xtst,_ = MNIST.testdata()\n",
    "        xtrn = Array{Float64, 3}(xtrn)\n",
    "        xtst = Array{Float64, 3}(xtst)\n",
    "\n",
    "        xtrn = resize_MNIST(xtrn, 1)\n",
    "        xtst = resize_MNIST(xtst, 1)\n",
    "        \n",
    "    end\n",
    "    \n",
    "elseif dataset_name == \"fashion\"\n",
    "    nc = 1\n",
    "    if use_saved_data\n",
    "\n",
    "        xtrn = npzread(\"Data/Fashion_MNIST_Train_Images.npy\")\n",
    "        xtrn = permutedims(xtrn, (3,4,2,1))\n",
    "        xtst = npzread(\"Data/Fashion_MNIST_Test_Images.npy\")\n",
    "        xtst = permutedims(xtst, (3,4,2,1))\n",
    "        \n",
    "    else\n",
    "        \n",
    "        xtrn,_ = FashionMNIST.traindata()\n",
    "        xtst,_ = FashionMNIST.testdata()\n",
    "        xtrn = Array{Float64, 3}(xtrn)\n",
    "        xtst = Array{Float64, 3}(xtst)\n",
    "\n",
    "        xtrn = resize_MNIST(xtrn, 1)\n",
    "        xtst = resize_MNIST(xtst, 1)\n",
    "\n",
    "    end\n",
    "    \n",
    "elseif dataset_name == \"cifar\"\n",
    "    nc = 3\n",
    "    xtrn,_= CIFAR10.traindata()\n",
    "    xtst,_ = CIFAR10.testdata()\n",
    "    xtrn = Array{Float64, 4}(xtrn)\n",
    "    xtst = Array{Float64, 4}(xtst)\n",
    "#     println(\"No implemented yet\")\n",
    "end\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtrn = minibatch(xtrn, batch_size; xsize = (28*28, nc,:), xtype = atype, shuffle = true)\n",
    "dtst = minibatch(xtst, batch_size; xsize = (28*28, nc,:), xtype = atype);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIREN_Layer_Weight_Init (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SIREN_Layer_Weight_Init(i, o; w0 = 30, is_first = false, bias::Bool = true, return_param = false)\n",
    "    if is_first\n",
    "       k = 1/i \n",
    "    else\n",
    "        k = sqrt(6/i)/w0\n",
    "    end\n",
    "    w = rand(Uniform(-k,k), o, i)\n",
    "    if bias \n",
    "        k_ = sqrt(1/i)\n",
    "        bias = rand(Uniform(-k_, k_), o, 1)\n",
    "        if return_param\n",
    "            return Param(w), Param(b)\n",
    "        else\n",
    "            return w, bias\n",
    "        end\n",
    "    end\n",
    "    if return_param\n",
    "       return Param(w) \n",
    "    else\n",
    "        return w\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weights (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gon_shape = [34, 256, 256, 256, 256, 1]\n",
    "\n",
    "function weights(gon_shape, w0)\n",
    "    theta = []  # Empty list initialization of weights\n",
    "    w,b = SIREN_Layer_Weight_Init(gon_shape[1], gon_shape[2]; is_first =true, w0 = w0)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    w, b = SIREN_Layer_Weight_Init(gon_shape[2], gon_shape[3]; w0 = w0)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    w, b = SIREN_Layer_Weight_Init(gon_shape[3], gon_shape[4]; w0 = w0)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    w, b = SIREN_Layer_Weight_Init(gon_shape[4], gon_shape[5]; w0 = w0)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    w, b = SIREN_Layer_Weight_Init(gon_shape[5], gon_shape[6]; w0 = w0)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    theta = map(a->convert(atype,a), theta)\n",
    "    return Param.(theta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batched_linear (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_mgrid(sidelen)\n",
    "    iterator = (range(-1,stop=1,length = sidelen))\n",
    "    return Array{Float64}(hcat([[i,j] for i = iterator, j = iterator]...)');\n",
    "end\n",
    "\n",
    "function batched_linear(theta, x_in; atype = KnetArray{Float32})\n",
    "#     \"\"\"\n",
    "#     multiply a weight matrix of size (O, I) with a batch of matrices \n",
    "#     of size (I, W, B) to have an output of size (O, W, B), \n",
    "#     where B is the batch size.\n",
    "    \n",
    "#     size(theta) = (O, I)\n",
    "#     size(x_in) = (O, W, B)\n",
    "#     \"\"\"\n",
    "    o = size(theta,1)\n",
    "    w = size(x_in, 2)\n",
    "    b = size(x_in, 3)\n",
    "    x_in_reshaped = reshape(x_in, size(x_in,1), w*b)\n",
    "    out = reshape(theta * x_in_reshaped, size(theta,1), w, b)\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_train (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function model_forw(theta, z, c; w0 = 30)\n",
    "   \n",
    "    z_ = copy(z)\n",
    "    z_ = permutedims(reshape(z_,64,1,1,num_latent),(4,3,2,1))\n",
    "    # The following line is the same for  :  hcat([z for _ = 1:size(c,2)]...)\n",
    "    # However it is more efficient while taking second order derivative of the loss.\n",
    "    # one_conv_weight is defined globally as convolution weights of all ones\n",
    "    z_rep = permutedims(conv4(one_conv_weight, z_)[:,1,:,:], (3,2,1))\n",
    "    z_in = cat(c, z_rep, dims = 3)\n",
    "    z_in = (permutedims(z_in, (3,2,1)))\n",
    "    z = batched_linear(theta[1], z_in) .+ theta[2]\n",
    "    z = sin.(w0 .* z)\n",
    "    \n",
    "    z = batched_linear(theta[3], z) .+ theta[4]\n",
    "    z = sin.(w0 .* z)\n",
    "    \n",
    "    z = batched_linear(theta[5], z) .+ theta[6]\n",
    "    z = sin.(w0 .* z)\n",
    "    \n",
    "    z = batched_linear(theta[7], z) .+ theta[8]\n",
    "    z = sin.(w0 .* z)\n",
    "    \n",
    "    z = batched_linear(theta[9], z) .+ theta[10]\n",
    "#     z = sin.(30 .* z)\n",
    "    z = permutedims(z, (2,1,3))\n",
    "end\n",
    "\n",
    "function loss(theta, z, x)\n",
    "    x_hat = model_forw(theta, z, c)\n",
    "    L = mean((x_hat- x).^2)\n",
    "#     L = mean(sum((x_hat - x).^2, dims = 1))\n",
    "end\n",
    "\n",
    "function loss_train(theta, x; batch_size = 64)\n",
    "    z = Param(atype(zeros(batch_size, 1, num_latent)))\n",
    "    derivative_origin = @diff loss(theta, z, x)\n",
    "    dz = grad(derivative_origin, z)\n",
    "    z = -dz\n",
    "    x_hat = model_forw(theta, z, c)\n",
    "    L = mean((x_hat- x).^2)\n",
    "#     L = mean(sum((x_hat - x).^2, dims = 1))\n",
    "    return L\n",
    "end\n",
    "\n",
    "function loss_train(theta, d::Data)\n",
    "     total_loss = 0\n",
    "    n_instance = 0\n",
    "    for x in d\n",
    "        batch_size_ = size(x,3)\n",
    "       total_loss += loss_train(theta, x; batch_size = batch_size_) * batch_size_\n",
    "        n_instance += batch_size_\n",
    "    end\n",
    "    total_loss / n_instance\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_reconstructed_images2 (generic function with 4 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function plot_reconstructed_images2(im_ori, im_rec, n_instances = 10, max_instance = 64, plot_size = (900,300))\n",
    "    k = rand(1:max_instance, n_instances)\n",
    "    ori_plot_list = reshape(im_ori[:,:,:,k[1]], (28, 28))\n",
    "    recon_plot_list = reshape(im_rec[:,:,:,k[1]], (28, 28))\n",
    "    for j in k[2:end]\n",
    "        ori_plot_list = hcat(ori_plot_list, reshape(im_ori[:,:,:,j], (28, 28)))\n",
    "        recon_plot_list = hcat(recon_plot_list, reshape(im_rec[:,:,:,j], (28, 28)))\n",
    "    end\n",
    "    p1 = plot(Matrix{Gray{Float32}}(ori_plot_list), title = \"Original Images\", size = (20,200),font =  \"Courier\", xtick = false, ytick = false)\n",
    "    p2 = plot(Matrix{Gray{Float32}}(recon_plot_list), title = \"Reconstructed Images\", font = \"Courier\", xtick = false, ytick = false)\n",
    "    plot(p1, p2, layout = (2,1), size = (900,300))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_conv_weight = atype(ones(1,1,1,784))\n",
    "\n",
    "batch_size = 64\n",
    "x = first(dtrn)\n",
    "mgrid = get_mgrid(28)\n",
    "c = atype(permutedims(repeat(mgrid,1,1,batch_size),(3,1,2)));\n",
    "c_copy = copy(c)\n",
    "c_copy[:,:,1] = c[:,:,2]\n",
    "c_copy[:,:,2] = c[:,:,1]\n",
    "c = c_copy\n",
    "num_latent = 32\n",
    "\n",
    "# define model weights\n",
    "theta = weights(gon_shape, 30);\n",
    "# Define Learning Rate and Number of Epochs\n",
    "lr = 2*1e-4\n",
    "n_epochs = 500\n",
    "# Specify the optimizer for each param\n",
    "for p in params(theta)\n",
    "    p.opt =  Knet.Adam(lr = lr, beta1 = 0.9, beta2 = 0.999)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: Now training of Implicit-GON is starting. We provide the parameters as the following\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: Dataset = fashion\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: num_latent = 32\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: lr = 0.0002\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: n_epochs = 500\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:31 | info | root]: Epoch : 0\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:32 | info | root]: Train Loss : 0.20152164\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:46:32 | info | root]: Test Loss : 0.20163457\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Initialize Empty Lists for both training and test losses\n",
    "trn_loss_list = Float64[]\n",
    "tst_loss_list = Float64[]\n",
    "\n",
    "\n",
    "# RECORD INITIAL LOSS VALUES\n",
    "epoch_loss_trn_ = loss_train(theta, dtrn)\n",
    "epoch_loss_tst_ = loss_train(theta, dtst)\n",
    "\n",
    "push!(trn_loss_list, epoch_loss_trn_)\n",
    "push!(tst_loss_list, epoch_loss_tst_)\n",
    "\n",
    "info(logger, (\"Now training of Implicit-GON is starting. We provide the parameters as the following\"))\n",
    "info(logger, \"Dataset = $dataset_name\")\n",
    "info(logger,\"num_latent = $num_latent\")\n",
    "info(logger, \"lr = $lr\")\n",
    "info(logger, \"n_epochs = $n_epochs\")\n",
    "\n",
    "info(logger, (\"Epoch : 0\"))\n",
    "info(logger, (\"Train Loss : $epoch_loss_trn_\"))\n",
    "info(logger, (\"Test Loss : $epoch_loss_tst_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣                    ┫ [0.20%, 1/500, 00:00/00:01, 572.95i/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06555851\n",
      "0.0650994\n",
      "0.06393664\n",
      "0.04932634\n",
      "0.042390026\n",
      "0.03834834\n",
      "0.036927365\n",
      "0.0339294\n",
      "0.036424104\n",
      "\u001b[32m[2021-12-20 04:50:23 | info | root]: Epoch : 1\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:50:23 | info | root]: Train Loss : 0.034436733\u001b[39m\n",
      "\u001b[32m[2021-12-20 04:50:23 | info | root]: Test Loss : 0.03444634\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣                    ┫ [0.40%, 2/500, 04:13/17:34:38, 253.11s/i] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028655665\n",
      "0.028749403\n",
      "0.028376698\n",
      "0.033038314\n",
      "0.026327541\n",
      "0.028540615\n",
      "0.028248327\n"
     ]
    }
   ],
   "source": [
    "########################################## CHANGE THE FOLLOWING LINES FOR CHECKPOINT ITERATION NUMBERS ############################\n",
    "# Define the step number of model save checkpoint\n",
    "model_save_checkpoint = 1\n",
    "logger_checkpoint = 1\n",
    "image_rec_checkpoint = 1\n",
    "\n",
    "x_ = first(dtst)\n",
    "for epoch in progress(1:n_epochs)\n",
    "    for (i,x) in enumerate(dtrn)\n",
    "        \n",
    "        derivative_model = @diff loss_train(theta, x)\n",
    "        \n",
    "#         if (i%100) == 0\n",
    "#            println(value(derivative_model)) \n",
    "#         end\n",
    "        for p in theta\n",
    "            dp = grad(derivative_model, p)\n",
    "            update!(value(p), dp, p.opt)\n",
    "        end\n",
    "    \n",
    "    end\n",
    "    \n",
    "    epoch_loss_trn = loss_train(theta, dtrn)\n",
    "    epoch_loss_tst = loss_train(theta, dtst)\n",
    "    push!(trn_loss_list, epoch_loss_trn)\n",
    "    push!(tst_loss_list, epoch_loss_tst)\n",
    "    \n",
    "    # Print losses to the logger file\n",
    "    if epoch % logger_checkpoint == 0\n",
    "        info(logger,\"Epoch : $epoch\")\n",
    "        info(logger,\"Train Loss : $epoch_loss_trn\")\n",
    "        info(logger,\"Test Loss : $epoch_loss_tst\")\n",
    "    end\n",
    "    \n",
    "    if ((epoch - 1) % image_rec_checkpoint == 0) || (epoch == n_epochs)\n",
    "        \n",
    "        z = Param(atype(zeros(batch_size, 1, num_latent)))\n",
    "        derivative_origin = @diff loss(theta, z, x_)\n",
    "        dz = grad(derivative_origin, z)\n",
    "        z = -dz\n",
    "        x_hat = model_forw(theta, z, c)\n",
    "        x_hat_ = Array{Float32}(reshape(x_hat, 28,28,1,64))\n",
    "        x__ = Array{Float32}(reshape(x_, 28,28,1,64))\n",
    "\n",
    "        (plot_reconstructed_images2(x__, x_hat_, 10, 64, (900,300)))\n",
    "        fig_name = \"Reconstructed_Imgs_ID\" * string(1000 + epoch) \n",
    "        savefig(joinpath(\"Results\", notebook_name, \"Images\", fig_name))\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # Save model at some steps\n",
    "    if (epoch % model_save_checkpoint == 0) || (epoch == n_epochs)\n",
    "        \n",
    "        model_id = 1000 + epoch\n",
    "        model_name = joinpath(\"Results\", notebook_name, \"Saved_Models\",\"Model_Base$model_id.jld2\")\n",
    "        w = Dict(:decoder => theta)\n",
    "        Knet.save(model_name,\"model\",w) \n",
    "        ### TO LOAD THE MODEL WEIGHTS, USE THE FOLLOWING\n",
    "        # w = Knet.load(model_name,\"model\",) # Ex: model_name = \"Results/Conv_AutoEncoder_Baseline_MNIST/Saved_Models/Model_Base1500.jld2\"\n",
    "        # theta = w[:decoder]\n",
    "#         Knet.save(joinpath(\"Results\", notebook_name,\"trn_loss_list.jld2\"),\"trn_loss_list\",trn_loss_list) \n",
    "#         Knet.save(joinpath(\"Results\", notebook_name,\"tst_loss_list.jld2\"),\"tst_loss_list\",tst_loss_list) \n",
    "        \n",
    "    end\n",
    "    Knet.save(joinpath(\"Results\", notebook_name,\"trn_loss_list.jld2\"),\"trn_loss_list\",trn_loss_list) \n",
    "    Knet.save(joinpath(\"Results\", notebook_name,\"tst_loss_list.jld2\"),\"tst_loss_list\",tst_loss_list) \n",
    "    \n",
    "end\n",
    "\n",
    "plot_loss_convergence(trn_loss_list[2:end], tst_loss_list[2:end]; title = \"Train & Test Loss w.r.t. Epochs\")\n",
    "fig_name = \"Train_and_test_loss\"\n",
    "savefig(joinpath(\"Results\", notebook_name, fig_name))\n",
    "\n",
    "plot_loss_convergence(trn_rec_loss_list[2:end], tst_rec_loss_list[2:end]; title = \"Train & Test Reconstruction Loss w.r.t. Epochs\")\n",
    "fig_name = \"Train_and_test_reconstruction_loss\"\n",
    "savefig(joinpath(\"Results\", notebook_name, fig_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
