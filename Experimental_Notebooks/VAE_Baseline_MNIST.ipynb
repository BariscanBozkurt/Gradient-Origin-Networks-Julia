{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnetArray{Float32, N} where N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set display width, load packages, import symbols\n",
    "ENV[\"COLUMNS\"]=72\n",
    "using Distributions\n",
    "using Interpolations\n",
    "using Knet: Knet, dir, accuracy, progress, sgd, load143, save, gc, Param, KnetArray, Data, minibatch, nll, relu, training, dropout,sigm # param, param0, xavier_uniform\n",
    "using Knet\n",
    "using Images\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using IterTools: ncycle, takenth\n",
    "using MLDatasets\n",
    "using Base.Iterators: flatten\n",
    "import CUDA # functional\n",
    "using ImageTransformations\n",
    "using Statistics\n",
    "using Memento\n",
    "using NPZ\n",
    "# using Interpolations\n",
    "atype=(CUDA.functional() ? KnetArray{Float32} : Array{Float32})\n",
    "# Atype = CuArray{Float32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const F = Float32\n",
    "params = Knet.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Memento.config!(\"info\"; fmt=\"[{date} | {level} | {name}]: {msg}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"PlotUtility.jl\")\n",
    "include(\"ImageUtility.jl\")\n",
    "include(\"TrainUtility.jl\")\n",
    "include(\"LayerUtility.jl\")\n",
    "include(\"LossUtility.jl\")\n",
    "\n",
    "using .PlotUtility\n",
    "using .ImageUtility\n",
    "using .TrainUtility\n",
    "using .LayerUtility\n",
    "using .LossUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mnist\"\n",
    "notebook_name = \"VAE_Baseline\" * \"_\" * dataset_name\n",
    "\n",
    "if !isdir(\"Results\")\n",
    "   mkdir(\"Results\") \n",
    "end\n",
    "if  !isdir(joinpath(\"Results\", notebook_name))\n",
    "    mkdir(joinpath(\"Results\", notebook_name))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Saved_Models\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Saved_Models\"))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Images\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Images\"))\n",
    "end\n",
    "\n",
    "if  !isdir(joinpath(\"Results\", notebook_name, \"Logger\"))\n",
    "    mkdir(joinpath(\"Results\", notebook_name, \"Logger\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(logger, DefaultHandler(joinpath(\"Results\", notebook_name, \"Logger\", \"logger.log\"),DefaultFormatter(\"[{date} | {level} | {name}]: {msg}\")));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_saved_data = false\n",
    "\n",
    "if dataset_name == \"mnist\"\n",
    "    \n",
    "    if use_saved_data\n",
    "        xtrn = npzread(\"Data/MNIST_Train_Images.npy\")\n",
    "        xtrn = permutedims(xtrn, (3,4,2,1))\n",
    "        xtst = npzread(\"Data/MNIST_Test_Images.npy\")\n",
    "        xtst = permutedims(xtst, (3,4,2,1))\n",
    "\n",
    "    else\n",
    "\n",
    "        xtrn,_ = MNIST.traindata()\n",
    "        xtst,_ = MNIST.testdata()\n",
    "        xtrn = Array{Float64, 3}(xtrn)\n",
    "        xtst = Array{Float64, 3}(xtst)\n",
    "\n",
    "        xtrn = resize_MNIST(xtrn, 32/28)\n",
    "        xtst = resize_MNIST(xtst, 32/28)\n",
    "        \n",
    "    end\n",
    "    \n",
    "elseif dataset_name == \"fashion\"\n",
    "    \n",
    "    if use_saved_data\n",
    "\n",
    "        xtrn = npzread(\"Data/Fashion_MNIST_Train_Images.npy\")\n",
    "        xtrn = permutedims(xtrn, (3,4,2,1))\n",
    "        xtst = npzread(\"Data/Fashion_MNIST_Test_Images.npy\")\n",
    "        xtst = permutedims(xtst, (3,4,2,1))\n",
    "        \n",
    "    else\n",
    "        \n",
    "        xtrn,_ = FashionMNIST.traindata()\n",
    "        xtst,_ = FashionMNIST.testdata()\n",
    "        xtrn = Array{Float64, 3}(xtrn)\n",
    "        xtst = Array{Float64, 3}(xtst)\n",
    "\n",
    "        xtrn = resize_MNIST(xtrn, 32/28)\n",
    "        xtst = resize_MNIST(xtst, 32/28)\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dtrn = minibatch(xtrn, batch_size; xsize = (32, 32, 1,:), xtype = atype, shuffle = true)\n",
    "dtst = minibatch(xtst, batch_size; xsize = (32, 32, 1,:), xtype = atype);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAxBJREFUeAHFwT1vFQQYBtDT9kGgolJAQUwsGgETjYpKQJR0gUFcMMHJ1ZXRTRf/gQmLkxtqahSVqBOaOJBoYAAlFIGCSMJXepuiUvp1HTo0tKG9txjfc6JYFItiUSyKRbEoFsWiWBSLYlEsikWxKBbFolgUi2JRLBapEzFjLVZgKUZxBZuwzrTLOI+GO0WxKBbFogVL0IGl6MYyLMcjZryGDViJy/gB+7ETo/gMH6HhTlEsikWxWEAXerECG7AFT+FR9JlrGGexCdvQwO/4GVfMFcWiWBSLeXThaRzAs7gPQSc6zdXEMXyCURzENTRwHsPmimJRLIrFPKZwHWNYhm53msQAbuJJPIBTOIQp0yYwiTFMmSuKRbEoFvNoooHPMWTa49iOMZzFe7iBDdiBU2hoXRSLYlEsFjCO73AGk9iKVejBVziCv3ACJzGqPVEsikWxaMGfuIomluAcXsJtTKCJmzihfVEsikWxaNG4adcwgC14HQdxERMWJ4pFsSgWbRpAPzaiD3twBDcwglvaE8WiWBSLNk3gBA5gK97FLpzEYRzDBJpaE8WiWBSLRbiFY3gfH2A3dmETPsZRDGtNFItiUSwWoYlhfImreAevYjcexMM4jCELi2JRLIrFIk1iCD9iCDuxD9twP3rwoYVFsSgWxeIeNHETv+ASNuN5vIBRfIMLmHJ3USyKRbG4Bx1YhV6sRQ860IWl6LCwKBbFolgsQvAQevEitmMjNqMD/+ACzqNpflEsikWxaEMXlmM9tuJNvII1CMZxHWdxCk0Li2JRLIpFCzrQidV4GXvxBtaZNolRDOJbfIrjWhPFolgUixb0oBdvYy/WY4kZA+jH1ziD21oXxaJYFIu76MJq7MczeALrsRLBLfyKL/ATBtHAbe2JYlEsisUsPXgOfdiMHehBN7owiT/wPfpxBtcxhqb2RbEoFsVilsewB29hDboxhNMYwUUcxXH8hnH3JopFsSgWs/yN0zhkxhDOYQSXcBpj/htRLIpFsZhlEIP+P1EsikWxKBbFolgUi2JRLIpFsX8B/++ldQuRwXUAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAxxJREFUeAHFwU9o1gUcB+Bnv33EGc20NikyBalVBIlEKpESHpL0oog3r506dOjQPejYJfEQQR0q6mQQGYFYYFCGB82i+Q/cElFEZyv/TIfvOuzQwfd9924Z3+eJYlEsikWxKBbFolgUi2JRLIpFsSgWxaJYFIv74GE8hRGzTuMMJswtikWxKBb/QR8GsQ1bscmsH3AAX5hbFItiUSwWqMEgNuNtjKBBC7uwBl9hCi2dRbEoFsViARoM4mW8h1Xoxy1cR4NhbMQxTKKlvSgWxaJYzFMwhM14B6uwCDOYwEH8jTfwOd7Hh7isvSgWxaJYzNMGvI7NeAzxrxUYxnmcwpN4FgM6i2JRLIrFPAxhA7bgcdzFSfyKjRjCNRzDIEawBH06i2JRLIpFjx7BDuzECkxhFPswjUncwEH8ghbexDNYjcu45V5RLIpFsejBUqzHTqzDFC7iAPZjGGO4hPO4jfNosAZbMI5x94piUSyKxRwavIjdeA6XcByncAg3MImz2luETfgG4+4VxaJYFIsu+jCIPdiGO9iPj3ABwV1zW47F2otiUSyKRRf9WIuXsAz7sBfn3D9RLIpFsehiAG/hUZzBzxjXmz40aKFPZ1EsikWx6OBBbMI6DOAExtDSuxZmMIbr2otiUSyKRQf9GMJS3MQRjJlbg1XYjrs4iU8xpr0oFsWiWHTQhwYNbmAME7prsAY7sBtX8CWOYFJ7USyKRbHowR+4immdLcUItmIrprAXX+MKWtqLYlEsikUXfWb9iTs6ewIb8SpewCQ+wMfmFsWiWBSLHqzDCM7iJho8gIcwgB3YhdUYxSf4WG+iWBSLYtHFjFnLsAdLcAyLsR6v4WkMYwJH8S0O610Ui2JRLHrQ4BWsxVUEK7EIDUZxFJ/hR0zrXRSLYlEsOriFE/gNz2MAQ1huVoNrOIy9OIdruG1+olgUi2LRwTTO4l1sx3asRD8u4hC+wwmMYsrCRLEoFsWigxau43tcwE9YbtYkfsdp/IUZCxfFolgUiy5mcBPHcdz/I4pFsSgWxaJYFItiUSyKRbEoFsWiWBSLYv8APr6cT/qPohMAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAnNJREFUeAHFwbtuFQQABuCv7V8uVWi5xECkponEqHhbHTTGMImTL+DiG+ju5GM4mfgIDqyGxjipGGIC2oDBpENRpGlKenNw4ND24PHQ5v++KIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyOCTBWbyOn7CCTXtFWZRFWewyjWPYwarxzeBdfI7P8C1W7RVlURZlscs5XMI2rhrfcbyMX/HQcFEWZVEWu5zDO9jAVeOZwFHM43lMGy7KoizKYpeLeBuLxncEC/gQ32EJa/YXZVEWZbHLMZzwdF7Ex5jDbaxi2/6iLMqiLAbM4iyOezqn8Qa28CPWDBdlURZlMWAeFzFjfMEJnMIWfsG64aIsyqIsBryABexgzXjO402cwSruYctwURZlURYDZjGHVSwZ3SRmcBKX8RGm8APuY8twURZlURb7eIDfPG4SE5jEFCYwgWk8g7dwGe/jVdzFl1jBtuGiLMqiLPYxjRMedwGzuIDXcAYzuIKj2MZdbGMDa7iBbU8WZVEWZTFgExu4hC/wiUcWMItpBOv4HYu4hZ+xhA/wKdaxhC1PFmVRFmUx4BqexRU8h9Me2cR13MId3MYq7uM+/sI8zmMKK3jov0VZlEVZDPgD3+AmTtlrBcu4hz/t9R5ewQNcN5ooi7Ioi12WsWw8c5jDMhaNJsqiLMriEPyNm0YTZVEWZXEIpnDEaKIsyqIsDtgEZnDOaKIsyqIsDtiOf00aTZRFWZTFITiJl4wmyqIsyuIQBMeMJsqiLMriAC1jGaeMLsqiLMriAN3A11jA90YTZVEWZXGA7uAr/0+URVmURVmURVmURVmURVmU/QN+uGOqCnTwRAAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAkNJREFUeAHFwT1rnQUABtCTy2NbqlJTzGAR0QoWhIKgIEYRPyAiIq5dHFydnN1cXP0F3RwrmKE6KKLSQahaRBw6RKlFRCF+NSZqvfc6dHBp9b03LzznRFmURVmURVmURVmURVmURVmURVmURVmURVmURVmUxchWcRx3Yorz2Mbfri/KoizKYkSH8QhexrPYxSl8hB3XF2VRFmUxopM4hacwwwxz/y3KoizKYkRHcBQ3GS7KoizKYiT34Tk8iF18jtfxBfbcWJRFWZTFCI7jJTyDVWzhLZzDX5i6sSiLsiiLEazjadyNHXyKs9jz/6IsyqIs9mEFB7GBe1xzEe/jG8NEWZRFWSxpglvwGNaxisv4EB8bLsqiLMpiSQdxP17DMezhHN7BJcNFWZRFWSxhgtvwJE4guIBNfGYxURZlURZLWMMLeAWH8BXewHv402KiLMqiLBZ0M57AqziKFXyPy7iCucVEWZRFWSzoBDZwzL/exSXMLC7KoizKYgFH8AAewhxTXMQn2LacKIuyKIsFrON53IWr+A6n8TWuWk6URVmUxQIexwYO4Gd8gDfxi+VFWZRFWQx0O9ZwCDP8irdxBTPLi7Ioi7IYYIIX8ShWXPMbvsTU/kRZlEVZDHAvHsYdmGILZ7CNuf2JsiiLshjgVqzhMHZxHqfxB+b2J8qiLMpigAmCCWbYwY/GEWVRFmUxwE/4FiexYlxRFmVRFgP8gDPYwgFcMJ4oi7IoiwF+xyY2jS/KoizKoizKoizKoizKoizK/gG2UF3zLBw2nwAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAspJREFUeAHFwbtrHAQcB/DPHd8Y0kSThhbBWjTVgrhUsUMk6h9QfLS0g4sOKuLq4uoqrk6Cjr5qhg5a8EEtIkocjJqIFE1VkGojxTZiHl4u5+Aod95dhN/nE8WiWBSLYlEsikWxKBbFolgUi2JRLIpFsSgWxaJYFItiUSyKRbEoFsWiWBSLYjGEBoIgaKKBFlpooIEW2nqLYlEsisUQJnEUx3APDmMUZ/EhghtxFkto6y6KRbEoFgO6BcfxOG7GOFYQzOF+NLCJKTyPDd1FsSgWxaJPTezHg3gSB7GCd7GAbTyEh3EAq1hDR29RLIpFsejT7TiOUziARbyMBfyCGTSxBy1cwkfY1lsUi2JRLPpwBKdwEtP4GG/jDLb84z7ci734Ge/hS7T1FsWiWBSLHpq4DU/gBCZwHq/gHLbQxDQewCH8jk/wBv7036JYFIti0UUDe/AUHsUoPsPr+ADbGMMMZnEnGljAa1jWnygWxaJYdNHENE5iEgt4Ce9jBDfgDryAuzCGLzCPc/oXxaJYFIsuGrgOUwjewhJmcQxzOIxpBB18jUVs6V8Ui2JRLLrooIVruB7P4RmMYhKjWMdVTGETS7hgMFEsikWx6GIHV/AinsUMJrGC81jEOp7GESzje2waTBSLYlEsuuhgHWdwDfvwF1bxA37FIezDCH7EbwYXxaJYFIsednAZb/q3CezHTRjBd7hscFEsikWxGFIbV7GDDi5i1eCiWBSLYjGkLVzEjt2JYlEsisWQJjCL2J0oFsWiWAxpBAfRtDtRLIpFsRjSJpaxg4bhRbEoFsViSJv4FpdwK/ZiHBsGE8WiWBSLIbVxBe/gMczhAj7FH/oXxaJYFItdaGEeR3E3HsEaPkdbf6JYFItisQttfIV5nMIsfsI3WNOfKBbFoljsQgcbeBWrOIFxjGFNf6JYFIti8T/YwGmcNrgoFsWi2N9EhIkrm4OwKgAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAy1JREFUeAHFwU1oFgQcB+Dndb+0nJ8zzT6MsLQSCbEQLTGxMMzLKC8SURGMoHO37t2qS3Tq0CU8FEhQgZoNtNIORlBqStg0M7ZmYtO15vZ28DCkd19m/J8nikWxKBbFolgUi2JRLIpFsSgWxaJYFItiUSyKRbEoFsXiBmhDMA9zMYRzGDW5KBbFolhcpwZmYREWYQEewHKcw4c4a3JRLIpFsZimBoJ2rMBLeBgrMAcNnMYcvIFRE4tiUSyKxTTNxzrsxDbMRXABPWjHbXgSu3AKo8YXxaJYFIspuhlP4Fmsw1LMRwO/4W18h050YTE60INR44tiUSyKxRQ9iE5sxwK0YRS9eBO7MYhNaCBoM7koFsWiWEzRWqzFIvyNPpzGbnyEs1iGuaYnikWxKBZT1Ifv0Yez6MFJfI5+jGAB5pueKBbFolhM0X70YTZ+wu8YcK2F6DA9USyKRbGYogF8bUywGDOMWYE7MIJLuIymiUWxKBbFYppmILgdO3ELGq5ahSXox2H8gBETi2JRLIrFNNyEJdiIF7AJDTTQRBuGsBdvYcTkolgUi2IxBW24C89jOxajHRdxDKuxEDMwjA48hJ8xbGJRLIpFsZhEsApd2IYB7MFRnMQwXscjmI2ZWI2XcQ5HMISm1qJYFItiMY4GZmELdmArGvgU7+MMruBFLMEMHMd53In1eA3dOIB+jGAAF4yJYlEsisU45mADXsGjOINufIwfsRRb0YWFOIxP8Avux3qsxHKswXlcxgHsMSaKRbEoFi20Yw1exRYcxwc4iEt4DBuxA1ewH7twEBfQgW6sx1PYjFk4iROuFcWiWBSLFpbhOTyNv7AXR3E3NuBxrEQv3sVn6MGQq3rRi0P4FpsxD1+g27WiWBSLYtHCPXgGDbShE524Fe24gl/xDt7DIJr+bRj7sM/4olgUi2LRwhD+QAdm4l5XncBhHMKX6MEgmq5fFItiUSxaOIIubMJ9GMBXOIZ+XMQlDKPpv4liUSyKRQsX8Q1OYR6G0Ys/MYKmGyeKRbEoFi00MYjT/n9RLIpFsSgWxaJYFItiUSyKRbEoFsWiWBSLYlEsikWxfwCTAKQcTJJ+WgAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAelJREFUeAHFwb2KlGcABtAz+LgRERU0wX8kisFFbAxY2sVbCaT2PrwPC8HKymARUlgImhAICQoGMdtEnWIJY3ZSbGE77/ctPOdEWZRFWZRFWZRFWZRFWZRFWZRFWZRFWZTFBKfxLa7hPZ7gHT4ZF2VRFmUxKLiOe7iDt/gBT/HRuCiLsiiLQUdwEdtY4Bwu4Ag+GhdlURZlMehfvMGv+Mp8URZlURaDVviAHQcjyqIsymLQFk7jooMRZVEWZTEoOIkzPruEc1hi15goi7Ioi0Er7OAPfG3fXfyND3hlTJRFWZTFoBV+xyN8Z99NXMePxkVZlEVZTLCDn7HA2jxRFmVRFjMssDBPlEVZlMVEa+xhbZ4oi7Ioi7Ioi7Ioi7Ioi7Ioi4kWWPjsEk4ZF2VRFmVxANa4gW08w9LmoizKoiwm2sUv+AaHcBa38BOe21yURVmUxURLPMEVHLLvMi7juc1FWZRFWUy0ixfYxRYWOIYTOIyVzURZlEVZTPQPHuB73MAXuIU7eIx3NhNlURZlMcMe/sRVbNm3NibKoizKYoYVHuI2jpsmyqIsymKG//ASr/EljhoXZVEWZTHDHv7CfZzHYfyGpc1FWZRFWcywxhKPTBdlURZlURZlURZlURZlURZlURZlURZl/wPDuUPpor8qhwAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAxhJREFUeAHFwc9r1wUcB+Bn+742f2yKpgU6jFFRWogFBQ0KBcGgjCDs3MFDh6BTl6BO/Qvd6tjRm4cID0aQYEViPyBtKiWzzVCnuDbn5qfD9zDGvtPPkHg/TxSLYlEsikWxKBbFolgUi2JRLIpFsSgWxaJYFItiUSyKxRoNYB0GsA170G+lu5jCz7hrdVEsikWxaGkQw9iFEWzGi3gfg1a6jW/xES5gDo2VolgUi2LR0i4cwmHsxSC2ow+NlYZwEJ/gU5zDXStFsSgWxaKlt/EunkQHfR5sEG/iOK7gupWiWBSLYtHSJdzAAOYxgW8stxPPYpclg+hYXRSLYlEsWvoOtzGCBdzEOfSh0bUFR3AUw1jEGZzDjN6iWBSLYtHSJG5gAA0WMWu5xzGHPl2LOI9JzOstikWxKBYtNZjDnN524hU8j/W6FnAS02j0FsWiWBSLh9CPHRjFy9iPfehgEdfwNW5ZXRSLYlEsWgqGsQnrdA3idbyDpzGEPsxgAj9gDo3VRbEoFsWihX6MYD8O4ildHbyAjiVz+Amf4RSm0VhdFItiUSxaGMXHeA1b0a+rD0FjyRRO4jgWcM/9RbEoFsWihQ42YAjrLNePe5bsxBiewxkPFsWiWBSLFq7gC1zARlzGgq4+NNiHAxjFZgxpJ4pFsSgWLczgFP5EcB2LlvseWzGKRntRLIpFsWhpFuNWtwkz1i6KRbEoFvfRwTbcxDwave3AITxj7aJYFIti0UMwjBG8iq8wgQVL+tDBMN7Ce9iDBg0a7USxKBbFoodHcAhHMYYJ3MKsJcFjOIAP8AT68S+mMaudKBbFolj0sBsfYjeCI3gJC5YMYwx7sRH9uIgTOIaz2oliUSyKRQ8drEd0vYG7aCzpxxA24BbO4kucwDUsaieKRbEoFj1cxOc4jDFssdwcbuBvjOMkfsRv+AeN9qJYFIti0cMkjmEcl3EQ23EVf+B3/IJpTOE8rmLe2kWxKBbFooc7+AuTmMSveBRTGMc4LmHGw4tiUSyKxSoa3MFpnPb/iWJRLIpFsSgWxaJYFItiUSyK/QdkGJ4ra7H8QQAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAb5JREFUeAHFwU1ujWEABtDjeuovbaJJNYQwaMTUBENbsABTO7ICEzMbaSQYVAyIRGKCNH5zW9prDd/73eQ5J8qiLMqiLMqiLMqiLMqiLMqiLMqiLMqiLMqiLMqiLAZtYBu38Rq/jImyKIuyGLDADh7hMZ7gLVami7Ioi7IYENzBU3zFLj5gabooi7IoixnOYAdXcB5L00VZlEVZzLTAwrgoi7Ioi5lW2MDCmCiLsiiLNbiGSzg0XZRFWZTFoBMc4xxuYdOYKIuyKIsBJ/iNz7iJyzhnTJRFWZTFgBMc4h2umyfKoizKYg3OGBdlURZlsQZb2DAmyqIsymIN7mHHmCiLsiiLQUsc4CE2sWFMlEVZlMWgI7zHKRbYwgUsTRNlURZlMegEP7DCWWziPJamibIoi7IY9Bef8RPbuIFtfDdNlEVZlMWgP9jHAe5jD7v4aJooi7Ioixn+YR97eIGXpouyKIuyWIMV/uHUdFEWZVEWa3ARd/EKn0wTZVEWZTFD8ACX8BvHpouyKIuymGGFI3zBNyxNF2VRFmUxwzGe4Sre4Mh0URZlURYzHOO5eaIsyqIsyqIsyqIsyqIsyqLsPwhtPJbX077HAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAtNJREFUeAHFwU9oFQQcB/CP23cbK3OjGFIY0Vp/0JQgFmGGZRhElxBi4KFO3aS8hceOXatDUBcPBZEgnSwoibA0oiIh8rAlKEqR21KDrb22dXiHIfO1956u3+cTxaJYFItiUSyKRbEoFsWiWBSLYlEsikWxKBb/g2cwjJ8w6VpRLIpFsVhHvXgQr+IWvI1J14piUSyKxTrpw904iHGcxKzVolgUi2KxTkZwEBNYxs+4aLUoFsWiWKyTQezAAE7jC5y1WhSLYlEsujCMcTyHN3EJi1Zsxh7ch9/wHs5gyWpRLIpFsejCGF7DQziCK5iz4h5MYBPewnHMuL4oFsWiWHRhEx7ACG5HrxUbMYYd6MEUptFwfVEsikWx6NCdeBiDmMEsFjX1YhSPYxAXcRYLWotiUSyKRQf68ChewAA+wxQWNG3Bs9iNaXyI7zGntSgWxaJYtKkfo9iNbTiDd/AHFjGE57EfW/Ap3rC2KBbFoli0oQ9jOIAX8SeO4RcsYQP2YgJb8SM+0p4oFsWiWKwh2IvX8Rj6Ne3CVRzGAHbiXkziCI5pTxSLYlEs/kMf9uAQHkE/rqKBJ7ANuzCMrdiIoziMhvZEsSgWxaKFW7ETh7AdS/gEJzCCfRjFXvRjAEt4Ei/jMC5ZWxSLYlEsWhjGPtyPL3EKJzCFzfgbL2ELerGMHtyBEcxrTxSLYlEsWuhBLz7A5/gOs5rmcBoNTSdwAQu4gOP4S3uiWBSLYtHCZRzFD5jGP5p6MIQx3IV5vI9vMYc5XNW+KBbFoli0cAXHrHYbtuNp9OAbfI1fdSeKRbEoFh3YgK3Yj6cwhQM4r3tRLIpFsejAEMaxCzP4GOfR0L0oFsWiWHRgAq9gGUfxLubdmCgWxaJYdGAWl9HAV/jdjYtiUSyKRQdOYR6DOOnmiGJRLIpFB87hnJsrikWxKBbFolgUi2JRLIpFsSgWxaJYFIti/wKUC4DCygategAAAABJRU5ErkJg\"></td></tr></tbody></table><div><small>(a vector displayed as a row to save space)</small></div>"
      ],
      "text/plain": [
       "10-element Vector{Matrix{Gray{Float32}}}:\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]\n",
       " [Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); … ; Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0); Gray{Float32}(0.0f0) Gray{Float32}(0.0f0) … Gray{Float32}(0.0f0) Gray{Float32}(0.0f0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1:10\n",
    "if use_saved_data\n",
    "    [Matrix{Gray{Float32}}(reshape(xtrn[:,:,:,j], (32, 32))) for j in k]\n",
    "else\n",
    "    [Matrix{Gray{Float32}}(reshape(xtrn[:,:,j], (32, 32))) for j in k]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weights (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function weights(nc, nz, ngf) \n",
    "    \n",
    "    # Encoding Weights\n",
    "    phi = [] # x -> z\n",
    "    \n",
    "    w,b = conv_weight_init(4, 4, nc, ngf; bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    push!(phi, bnparams(ngf))\n",
    "    \n",
    "    w,b = conv_weight_init(4, 4, ngf, ngf * 2; bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    push!(phi, bnparams(ngf * 2))\n",
    "    \n",
    "    w,b = conv_weight_init(4, 4, ngf * 2, ngf * 4; bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    push!(phi, bnparams(ngf * 4))\n",
    "    \n",
    "    w,b = conv_weight_init(4, 4, ngf * 4, nz; bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    \n",
    "    w, b = linlayer_weight_init(nz, nz;bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    \n",
    "    w, b = linlayer_weight_init(nz, nz;bias = true, return_param = false)\n",
    "    push!(phi, w)\n",
    "    push!(phi, b)\n",
    "    \n",
    "    # Decoding Weights\n",
    "    theta = [] # z -> x\n",
    "    \n",
    "    w, b = deconv_weight_init(4, 4, nz, ngf * 4;bias = true, return_param = false)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    push!(theta, (bnparams(ngf * 4))) # Batch Normalization\n",
    "    \n",
    "    w, b = deconv_weight_init(4, 4, ngf * 4, ngf * 2;bias= true, return_param = false)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    push!(theta, (bnparams(ngf * 2)))\n",
    "    \n",
    "    w, b = deconv_weight_init(4, 4, ngf * 2, ngf;bias = true, return_param = false)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    push!(theta, (bnparams(ngf)))\n",
    "    \n",
    "    w,b = deconv_weight_init(4,4,ngf, 1;bias = true, return_param = false)\n",
    "    push!(theta, w)\n",
    "    push!(theta, b)\n",
    "    \n",
    "    phi = map(a->convert(atype,a), phi)\n",
    "    theta = map(a->convert(atype,a), theta)\n",
    "#     theta = map(a->convert(Param,a), theta)\n",
    "    \n",
    "    return Param.(theta), Param.(phi)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode_decode (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reparametrize(mu, logvar)\n",
    "    \n",
    "    std = exp.(0.5 .* logvar)\n",
    "    epsilon = convert(atype, randn(F, size(mu)))\n",
    "    z = mu .+ epsilon .* std\n",
    "    \n",
    "    return z\n",
    "end\n",
    "\n",
    "moments1 = bnmoments()\n",
    "moments2 = bnmoments()\n",
    "moments3 = bnmoments()\n",
    "moments4 = bnmoments()\n",
    "moments5 = bnmoments()\n",
    "moments6 = bnmoments()\n",
    "\n",
    "# DEFINE ENCODING FUNCTION (MODEL)\n",
    "function encode(phi, x; batch_size = 64, training = true)\n",
    "    \n",
    "    z =  conv4(phi[1], x, mode = 1, stride = 2, padding = 1) .+ phi[2]\n",
    "    z = mybatchnorm(z, moments1, phi[3]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z = conv4(phi[4], z, mode = 1, stride = 2, padding = 1) .+ phi[5]\n",
    "    z = mybatchnorm(z, moments2, phi[6]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z =  conv4(phi[7], z, mode = 1, stride = 2, padding = 1) .+ phi[8]\n",
    "    z = mybatchnorm(z, moments3, phi[9]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z = conv4(phi[10], z, mode = 1) .+ phi[11]\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    #z = reshape(z, (size(z,3), size(z,4)))\n",
    "    z = reshape(z, (nz, batch_size))\n",
    "    mu = phi[12] * z .+ phi[13]\n",
    "    logvar = phi[14] * z .+ phi[15]\n",
    "    \n",
    "    z = reparametrize(mu, logvar)\n",
    "    \n",
    "    return z, mu, logvar\n",
    "end\n",
    "\n",
    "function decode(theta, z; batch_size = 64, training = true)\n",
    "        \n",
    "    z = reshape(z, (1, 1, nz, batch_size))\n",
    "    z = deconv4(theta[1], z, mode = 1) .+ theta[2]\n",
    "    z = mybatchnorm(z, moments4, theta[3]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z = deconv4(theta[4], z, stride = 2, padding = 1, mode = 1) .+ theta[5]\n",
    "    z = mybatchnorm(z, moments5, theta[6]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z = deconv4(theta[7], z, stride = 2, padding = 1, mode = 1) .+ theta[8]\n",
    "    z = mybatchnorm(z, moments6, theta[9]; training = training)\n",
    "    z = Knet.elu.(z)\n",
    "    \n",
    "    z = deconv4(theta[10], z, stride = 2, padding = 1, mode = 1) .+ theta[11]\n",
    "    x_hat = Knet.sigm.(z)\n",
    "\n",
    "    return x_hat\n",
    "\n",
    "end\n",
    "\n",
    "function VAEsample(theta, nz, batch_size; training = true)\n",
    "\n",
    "    z = atype(randn(1,1, nz, batch_size))\n",
    "    \n",
    "    x_hat = decode(theta, z; batch_size = batch_size, training = training)\n",
    "\n",
    "    return x_hat\n",
    "end\n",
    "\n",
    "function encode_decode(theta, phi, x; training = true)\n",
    "    batch_size = size(x,4)\n",
    "    z, mu, logvar = encode(phi, x; batch_size = batch_size, training = training)\n",
    "    x_hat = decode(theta, z; batch_size = batch_size, training = training)\n",
    "    return x_hat, mu, logvar\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec_loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(theta, phi, x)\n",
    "    \n",
    "#     batch_size = size(x,4) \n",
    "    z, mu, logvar = encode(phi, x)\n",
    "    x_hat = decode(theta, z)\n",
    "    L = BCE(x, x_hat) + KLD(mu, logvar)\n",
    "    return L\n",
    "    \n",
    "end\n",
    "\n",
    "function loss(theta, phi, d::Data)\n",
    "    \n",
    "    total_loss = 0\n",
    "    n_instance = 0\n",
    "    for x in d\n",
    "        total_loss += loss(theta, phi, x) * size(x,4)\n",
    "        n_instance += size(x,4)\n",
    "    end\n",
    "\n",
    "    total_loss /= n_instance\n",
    "end\n",
    "\n",
    "## Sum Squared Error Definition\n",
    "rec_loss(m, theta, phi, x) = sum((x - m(theta, phi, x)[1]).^2) / size(x,4)\n",
    "\n",
    "function rec_loss(m, theta, phi, d::Data)\n",
    "    total_loss = 0\n",
    "    n_instance = 0\n",
    "   for x in d\n",
    "        total_loss += rec_loss(m, theta, phi, x) * size(x,4)\n",
    "        n_instance += size(x,4)\n",
    "    end\n",
    "    \n",
    "    total_loss /= n_instance\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 256\n",
    "ngf = 16\n",
    "nc = 1\n",
    "\n",
    "x = first(dtrn)\n",
    "\n",
    "theta, phi = weights(nc, nz,  ngf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842.9994993209839"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(theta, phi, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846.2343272552165"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(theta, phi, dtrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T(846.5265989303589)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@diff loss(theta, phi, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T(315.73874)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@diff rec_loss(encode_decode, theta, phi, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T(973.0028705596924)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@diff loss(theta, phi, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-12-15 14:56:39 | info | root]: Epoch : 0\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:56:39 | info | root]: Train Loss : 782.5799660573265\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:56:39 | info | root]: Test Loss : 782.869368540935\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:56:39 | info | root]: Train Reconstruction Loss : 252.58781\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:56:39 | info | root]: Test Reconstruction Loss : 252.87437 \u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣                    ┫ [0.50%, 5/1000, 00:58/03:14:14, 14.08s/i] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-12-15 14:57:52 | info | root]: Epoch : 5\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:57:52 | info | root]: Train Loss : 155.18710548417164\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:57:52 | info | root]: Test Loss : 157.16693395223373\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:57:52 | info | root]: Train Reconstruction Loss : 9.730999\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:57:52 | info | root]: Test Reconstruction Loss : 10.519631 \u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣▏                   ┫ [0.90%, 9/1000, 01:55/03:32:41, 14.30s/i] "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] forw(::Function, ::Function, ::Vararg{Any, N} where N; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ AutoGrad ~/.julia/packages/AutoGrad/TTpeo/src/core.jl:64",
      "  [2] forw",
      "    @ ~/.julia/packages/AutoGrad/TTpeo/src/core.jl:65 [inlined]",
      "  [3] broadcasted(#unused#::typeof(+), x1::KnetArray{Float32, 4}, x2::Param{KnetArray{Float32, 4}})",
      "    @ AutoGrad ./none:0",
      "  [4] encode(phi::Vector{Param}, x::KnetArray{Float32, 4}; batch_size::Int64, training::Bool)",
      "    @ Main ./In[10]:20",
      "  [5] encode",
      "    @ ./In[10]:20 [inlined]",
      "  [6] loss(theta::Vector{Param}, phi::Vector{Param}, x::KnetArray{Float32, 4})",
      "    @ Main ./In[11]:4",
      "  [7] loss(theta::Vector{Param}, phi::Vector{Param}, d::Data{KnetArray{Float32, N} where N})",
      "    @ Main ./In[11]:16",
      "  [8] top-level scope",
      "    @ In[16]:85",
      "  [9] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "nz = 48\n",
    "ngf = 16\n",
    "# nc = 1\n",
    "\n",
    "# first batch of the test dataset\n",
    "x_test_first = first(dtst);\n",
    "\n",
    "# Initialize random model weights\n",
    "theta, phi = weights(nc, nz, ngf);\n",
    "\n",
    "# Define Learning Rate and Number of Epochs\n",
    "lr = 2*1e-4\n",
    "n_epochs = 5 # MAKE THIS 1000\n",
    "\n",
    "# Specify the optimizer for each param\n",
    "for p in params(theta)\n",
    "    p.opt =  Knet.Adam(lr = lr, beta1 = 0.9, beta2 = 0.999)\n",
    "end\n",
    "\n",
    "for p in params(phi)\n",
    "    p.opt =  Knet.Adam(lr = lr, beta1 = 0.9, beta2 = 0.999)\n",
    "end\n",
    "\n",
    "# Initialize Empty Lists for both training and test losses\n",
    "trn_loss_list = Float64[]\n",
    "tst_loss_list = Float64[]\n",
    "trn_rec_loss_list = Float64[]\n",
    "tst_rec_loss_list = Float64[]\n",
    "\n",
    "# RECORD INITIAL LOSS VALUES\n",
    "epoch_loss_trn_ = loss(theta, phi, dtrn)\n",
    "epoch_loss_tst_ = loss(theta, phi, dtst)\n",
    "epoch_rec_loss_trn_ = rec_loss(encode_decode, theta, phi, dtrn)\n",
    "epoch_rec_loss_tst_ = rec_loss(encode_decode, theta, phi, dtst)\n",
    "\n",
    "push!(trn_loss_list, epoch_loss_trn_)\n",
    "push!(tst_loss_list, epoch_loss_tst_)\n",
    "push!(trn_rec_loss_list, epoch_rec_loss_trn_)\n",
    "push!(tst_rec_loss_list, epoch_rec_loss_tst_)\n",
    "\n",
    "# println(\"Epoch : \", 0)\n",
    "# println(\"Train Loss : \",epoch_loss_trn_)\n",
    "# println(\"Test Loss : \", epoch_loss_tst_)\n",
    "# println(\"Train Reconstruction Loss : \", epoch_rec_loss_trn_)\n",
    "# println(\"Test Reconstruction Loss : \", epoch_rec_loss_tst_)\n",
    "\n",
    "info(logger, (\"Epoch : 0\"))\n",
    "info(logger, (\"Train Loss : $epoch_loss_trn_\"))\n",
    "info(logger, (\"Test Loss : $epoch_loss_tst_\"))\n",
    "info(logger, (\"Train Reconstruction Loss : $epoch_rec_loss_trn_\"))\n",
    "info(logger, (\"Test Reconstruction Loss : $epoch_rec_loss_tst_ \\n\"))\n",
    "\n",
    "# Define the step number of model save checkpoint\n",
    "model_save_checkpoint = 1 # I RECOMMEND DOING THIS 100\n",
    "logger_checkpoint = 1\n",
    "image_rec_checkpoint = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in progress(1:n_epochs)\n",
    "    \n",
    "    # # DECREASE LEARNING RATE AFTER 50 EPOCHS\n",
    "    # if epoch > 50\n",
    "    #    lr = 1e-4 \n",
    "    # end\n",
    "    \n",
    "    for (i,x) in enumerate(dtrn)\n",
    "        \n",
    "        # CALCULATE THE GRADIENT OF THE LOSS FUNCTION W.R.T. MODEL WEIGHTS\n",
    "        derivative_model = @diff loss(theta, phi, x)\n",
    "        \n",
    "        # UPDATE MODEL WEIGHTS WITH ADAM OPTIMIZER\n",
    "        for p in theta\n",
    "            dp = grad(derivative_model, p)\n",
    "            update!(value(p), dp, p.opt)\n",
    "        end\n",
    "        \n",
    "        for p in phi\n",
    "            dp = grad(derivative_model, p)\n",
    "            update!(value(p), dp, p.opt)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # Record Training and Test Losses\n",
    "    epoch_loss_trn = loss(theta, phi, dtrn)\n",
    "    epoch_loss_tst = loss(theta, phi, dtst)\n",
    "    epoch_rec_loss_trn = rec_loss(encode_decode, theta, phi, dtrn)\n",
    "    epoch_rec_loss_tst = rec_loss(encode_decode, theta, phi, dtst)\n",
    "    \n",
    "    push!(trn_loss_list, epoch_loss_trn)\n",
    "    push!(tst_loss_list, epoch_loss_tst)\n",
    "    push!(trn_rec_loss_list, epoch_rec_loss_trn)\n",
    "    push!(tst_rec_loss_list, epoch_rec_loss_tst)\n",
    "    \n",
    "#     println(\"Epoch : \", epoch)\n",
    "#     println(\"Train Loss : \",epoch_loss_trn)\n",
    "#     println(\"Test Loss : \", epoch_loss_tst)\n",
    "#     println(\"Train Reconstruction Loss : \", epoch_rec_loss_trn)\n",
    "#     println(\"Test Reconstruction Loss : \", epoch_rec_loss_tst)\n",
    "    \n",
    "    # Print losses to the logger file\n",
    "    if epoch % logger_checkpoint == 0\n",
    "        info(logger,\"Epoch : $epoch\")\n",
    "        info(logger,\"Train Loss : $epoch_loss_trn\")\n",
    "        info(logger,\"Test Loss : $epoch_loss_tst\")\n",
    "        info(logger,\"Train Reconstruction Loss : $epoch_rec_loss_trn\")\n",
    "        info(logger,\"Test Reconstruction Loss : $epoch_rec_loss_tst \\n\")\n",
    "    end\n",
    "    \n",
    "    # Save Model Weights \n",
    "    if epoch % model_save_checkpoint == 0\n",
    "        model_id = 1000 + epoch\n",
    "        model_name = joinpath(\"Results\", notebook_name, \"Saved_Models\",\"Model_VAE$model_id.jld2\")\n",
    "        #Knet.save(model_name,\"model\",theta) \n",
    "        w = Dict(:encoder => phi, :decoder => theta)\n",
    "        Knet.save(model_name,\"model\",w) \n",
    "        ### TO LOAD THE MODEL WEIGHTS, USE THE FOLLOWING\n",
    "        # w = Knet.load(model_name,\"model\",) # Ex: model_name = \"Results/Conv_AutoEncoder_Baseline_MNIST/Saved_Models/Model_Base1500.jld2\"\n",
    "        # theta = w[:decoder]\n",
    "        # phi = w[:encoder]\n",
    "    end\n",
    "    \n",
    "#     if (epoch-1) % image_rec_checkpoint == 0 \n",
    "        \n",
    "#         x_hat = encode_decode(theta, phi, x_test_first)[1]\n",
    "#         plot_reconstructed_images(x_test_first, x_hat, 10, batch_size, (900,300))\n",
    "#         fig_name = \"Reconstructed_Imgs_ID\" * string(1000 + epoch) \n",
    "#         savefig(joinpath(\"Results\", notebook_name, \"Images\", fig_name))\n",
    "\n",
    "#         x_sampled = VAEsample(theta, nz, 64)\n",
    "#         plot_image_grid(x_sampled; grid_x_size = 8, grid_y_size = 8, title = \"VAE Sampled Images\")\n",
    "#         fig_name = \"VAEGON_Sampled_Imgs_ID\" * string(1000 + epoch) \n",
    "#         savefig(joinpath(\"Results\", notebook_name, \"Images\", fig_name))\n",
    "\n",
    "#     end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: Now training is done. Recall the parameters as the following\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: Dataset = mnist\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: nz = 48\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: ngf = 16\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: nc = 1\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: lr = 0.0002\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: n_epochs = 1000\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: Training is done!\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:47 | info | root]: We will report the last loss values for both training and test sets.\u001b[39m\n",
      "\n",
      "\u001b[32m[2021-12-15 14:58:52 | info | root]: Train Loss : 151.599184901381\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:52 | info | root]: Test Loss : 153.6559821886894\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:52 | info | root]: Train Reconstruction Loss : 8.730395\u001b[39m\n",
      "\u001b[32m[2021-12-15 14:58:52 | info | root]: Test Reconstruction Loss : 9.444819 \u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### LAST LOGGERS\n",
    "info(logger, (\"Now training is done. Recall the parameters as the following\"))\n",
    "info(logger, \"Dataset = $dataset_name\")\n",
    "info(logger,\"nz = $nz\")\n",
    "info(logger,\"ngf = $ngf\")\n",
    "info(logger, \"nc = $nc\")\n",
    "info(logger, \"lr = $lr\")\n",
    "info(logger, \"n_epochs = $n_epochs\")\n",
    "\n",
    "# plot_loss_convergence(trn_loss_list[2:end], tst_loss_list[2:end]; title = \"Train & Test Loss w.r.t. Epochs\")\n",
    "# fig_name = \"Train_and_test_loss\"\n",
    "# savefig(joinpath(\"Results\", notebook_name, fig_name))\n",
    "\n",
    "# plot_loss_convergence(trn_rec_loss_list[2:end], tst_rec_loss_list[2:end]; title = \"Train & Test Reconstruction Loss w.r.t. Epochs\")\n",
    "# fig_name = \"Train_and_test_reconstruction_loss\"\n",
    "# savefig(joinpath(\"Results\", notebook_name, fig_name))\n",
    "\n",
    "info(logger, \"Training is done!\")\n",
    "info(logger, \"We will report the last loss values for both training and test sets.\\n\")\n",
    "\n",
    "# Record Training and Test Losses\n",
    "epoch_loss_trn = loss(theta, phi, dtrn)\n",
    "epoch_loss_tst = loss(theta, phi, dtst)\n",
    "epoch_rec_loss_trn = rec_loss(encode_decode, theta, phi, dtrn)\n",
    "epoch_rec_loss_tst = rec_loss(encode_decode, theta, phi, dtst)\n",
    "\n",
    "info(logger,\"Train Loss : $epoch_loss_trn\")\n",
    "info(logger,\"Test Loss : $epoch_loss_tst\")\n",
    "info(logger,\"Train Reconstruction Loss : $epoch_rec_loss_trn\")\n",
    "info(logger,\"Test Reconstruction Loss : $epoch_rec_loss_tst \\n\")\n",
    "\n",
    "Knet.save(joinpath(\"Results\", notebook_name,\"trn_loss_list.jld2\"),\"trn_loss_list\",trn_loss_list) \n",
    "Knet.save(joinpath(\"Results\", notebook_name,\"tst_loss_list.jld2\"),\"tst_loss_list\",tst_loss_list) \n",
    "Knet.save(joinpath(\"Results\", notebook_name,\"trn_rec_loss_list.jld2\"),\"trn_rec_loss_list\",trn_rec_loss_list) \n",
    "Knet.save(joinpath(\"Results\", notebook_name,\"tst_rec_loss_list.jld2\"),\"tst_rec_loss_list\",tst_rec_loss_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
